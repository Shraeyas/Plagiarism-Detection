{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process the 20 Newsgroup dataset\n",
    "\n",
    "##### Download the Original 20 Newsgroups data set from : http://qwone.com/~jason/20Newsgroups/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the path of all files in the 20 Newsgroup dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "dataset_directory = os.path.join (os.getcwd (), \"dataset_20news\")\n",
    "def get_files () :\n",
    "    for (root, dirs, file) in os.walk (dataset_directory) :\n",
    "        for f in file :\n",
    "            if \".ipynb_checkpoints\" not in root :\n",
    "                files.append (os.path.join (root, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D:\\\\CS Projects\\\\Plagiarism-Detection\\\\dataset_20news\\\\20_newsgroups\\\\comp.os.ms-windows.misc\\\\10000', 'D:\\\\CS Projects\\\\Plagiarism-Detection\\\\dataset_20news\\\\20_newsgroups\\\\comp.os.ms-windows.misc\\\\10001', 'D:\\\\CS Projects\\\\Plagiarism-Detection\\\\dataset_20news\\\\20_newsgroups\\\\comp.os.ms-windows.misc\\\\10002']\n",
      "Total no. of files :  19997\n"
     ]
    }
   ],
   "source": [
    "get_files()\n",
    "print (files[2000:2003])\n",
    "print (\"Total no. of files : \", len (files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the raw text in all the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_text = []\n",
    "for file in files :\n",
    "    files_text.append (open (os.path.join (\"dataset\", file), 'r', encoding = \"cp437\").read ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19997\n"
     ]
    }
   ],
   "source": [
    "print (len (files_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Pre-processing on the data\n",
    "```\n",
    "1. Remove Stopwords\n",
    "2. Remove any special characters\n",
    "3. Convert the sentence to Lowercase\n",
    "4. Tokenize the sentence\n",
    "5. Perform Stemming\n",
    "6. Remove words that contain only digits\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Shraeyas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Shraeyas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "nltk.download ('punkt')\n",
    "nltk.download ('stopwords')\n",
    "lancaster = LancasterStemmer()\n",
    "\n",
    "def pre_process (_sentences) :\n",
    "    en_stops = set (stopwords.words ('english')) # Remove Stopwords\n",
    "    processed_sentences = []\n",
    "\n",
    "    for sentence in _sentences :\n",
    "        sentence = re.sub (r'[^\\w\\s]','', sentence) # Remove special characters\n",
    "        lower_sen = sentence.lower () # Convert the sentence to Lowercase\n",
    "        token_sen = word_tokenize (lower_sen) # Tokenize the sentence\n",
    "\n",
    "        new_sentence = []\n",
    "        for word in token_sen:\n",
    "            if word not in en_stops and word.isnumeric() == False : # Remove words that contain only digits\n",
    "                word = lancaster.stem (word) # Perform Stemming\n",
    "                new_sentence.append (word)\n",
    "\n",
    "        if len (new_sentence) > 3 :\n",
    "            processed_sentences.append (new_sentence)\n",
    "    return processed_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for file_text in files_text :\n",
    "    sentences = file_text.split (\".\")\n",
    "    documents.append (pre_process (sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a final Dataframe for the pre-processed documents\n",
    "##### The dataframe structure will be of the format :\n",
    "```\n",
    "document_id | sentence\n",
    "---------------------------------------------------\n",
    " 0          | statement with comma separated words\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_df = []\n",
    "for i, document in enumerate (documents) :\n",
    "    for sentence in document :\n",
    "        sen = \" \".join (sentence)\n",
    "        document_df.append ([i, sen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_df = pd.DataFrame (document_df, columns = [\"document_id\", \"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ath faq ath resourc sum book address mus anyth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>uk dat mon mar gmt expir thu apr gmt followupt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>ath distribut world org mant consult cambridg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>uk lin archivenam atheismresourc altatheismarc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>ath resourc address ath org us freedom relig f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document_id                                          sentences\n",
       "0            0  ath faq ath resourc sum book address mus anyth...\n",
       "1            0  uk dat mon mar gmt expir thu apr gmt followupt...\n",
       "2            0      ath distribut world org mant consult cambridg\n",
       "3            0  uk lin archivenam atheismresourc altatheismarc...\n",
       "4            0  ath resourc address ath org us freedom relig f..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_df.head ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the final Processed Dataset to disk as a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_df.to_csv (os.path.join (\"dataset_processed\", \"documents_processed.csv\"), index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
